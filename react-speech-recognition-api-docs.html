<!DOCTYPE html><html><head><meta charSet="utf-8"/><title>React Speech Recognition API docs</title><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="icon" href="/favicon.ico"/><meta name="next-head-count" content="4"/><link rel="preload" href="/_next/static/css/fcd984c04d57235acdaa.css" as="style"/><link rel="stylesheet" href="/_next/static/css/fcd984c04d57235acdaa.css" data-n-g=""/><link rel="preload" href="/_next/static/css/804ad107cac14cc49c60.css" as="style"/><link rel="stylesheet" href="/_next/static/css/804ad107cac14cc49c60.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-a40ef1678bae11e696dba45124eadd70.js"></script><script src="/_next/static/chunks/webpack-f47d69457824065d04c3.js" defer=""></script><script src="/_next/static/chunks/framework-1a85486469afb3278dba.js" defer=""></script><script src="/_next/static/chunks/main-75772fcf59f94cd717f4.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6f1364eda477534ba135.js" defer=""></script><script src="/_next/static/chunks/pages/react-speech-recognition-api-docs-5c79b6ac170552f948be.js" defer=""></script><script src="/_next/static/vDOyqE9dTMogwuLDZy0OQ/_buildManifest.js" defer=""></script><script src="/_next/static/vDOyqE9dTMogwuLDZy0OQ/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="Layout_Layout__2MSiz"><main><h1>React Speech Recognition API docs</h1><div class="Article_Article__2h4f-"><a href="/">Back to home</a><div class="Article_Article__content__Vy2r-"><h2>Interface</h2>
<ul>
<li><a href="#useSpeechRecognition">useSpeechRecognition</a></li>
<li><a href="#SpeechRecognition">SpeechRecognition</a></li>
</ul>
<p><a name="useSpeechRecognition"></a></p>
<h2>useSpeechRecognition</h2>
<p>React hook for consuming speech recorded by the microphone. Import with:</p>
<pre><code>import { useSpeechRecognition } from 'react-speech-recognition'
</code></pre>
<h3>Input props</h3>
<p>These are passed as an object argument to <code>useSpeechRecognition</code>:</p>
<pre><code>useSpeechRecognition({ transcribing, clearTranscriptOnListen, commands })
</code></pre>
<h4>transcribing [bool]</h4>
<p>Is this component collecting a transcript or not? This is independent of the global <code>listening</code> state of the microphone. <code>true</code> by default.</p>
<h4>clearTranscriptOnListen [bool]</h4>
<p>Does this component clear its transcript when the microphone is turned on? Has no effect when continuous listening is enabled. <code>true</code> by default.</p>
<h4>commands [list]</h4>
<p>See <a href="/react-speech-recognition#commands">Commands</a>.</p>
<h3>Output state</h3>
<p>These are returned from <code>useSpeechRecognition</code>:</p>
<pre><code>  const {
    transcript,
    interimTranscript,
    finalTranscript,
    resetTranscript,
    listening,
    browserSupportsSpeechRecognition,
    isMicrophoneAvailable,
  } = useSpeechRecognition()
</code></pre>
<h4>transcript [string]</h4>
<p>Transcription of all speech that has been spoken into the microphone. Is equivalent to the final transcript followed by the interim transcript, separated by a space.</p>
<h4>resetTranscript [function]</h4>
<p>Sets <code>transcript</code> to an empty string.</p>
<h4>listening [bool]</h4>
<p>If true, the Web Speech API is listening to speech from the microphone.</p>
<h4>interimTranscript [string]</h4>
<p>Transcription of speech that the Web Speech API is still processing (i.e. it's still deciding what has just been spoken).</p>
<p>For the current words being spoken, the interim transcript reflects each successive guess made by the transcription algorithm. When the browserâ€™s confidence in its guess is maximized, it is added to the final transcript.</p>
<p>The difference between interim and final transcripts can be illustrated by an example over four iterations of the transcription algorithm:</p>
<img src="images/transcript-table.png" width="300" alt="A series of transcripts that grow as more speech is transcribed">
<h4>finalTranscript [string]</h4>
<p>Transcription of speech that the Web Speech API has finished processing.</p>
<h4>browserSupportsSpeechRecognition [bool]</h4>
<p>The Web Speech API is not supported on all browsers, so it is recommended that you render some fallback content if it is not supported by the user's browser:</p>
<pre><code>if (!browserSupportsSpeechRecognition) {
  // Render some fallback content
}
</code></pre>
<h4>browserSupportsContinuousListening [bool]</h4>
<p>Continuous listening is not supported on all browsers, so it is recommended that you apply some fallback behaviour if your web app uses continuous listening and is running on a browser that doesn't support it:</p>
<pre><code>if (browserSupportsContinuousListening) {
  SpeechRecognition.startListening({ continuous: true })
} else {
  // Fallback behaviour
}
</code></pre>
<h4>isMicrophoneAvailable [bool]</h4>
<p>The user has to give permission for their microphone to be used before transcription can begin. They are asked for permission when <code>react-speech-recognition</code> first tries to start listening. This state will become <code>false</code> if they deny access. In this case, it's advised that you disable voice-driven features and indicate that microphone access is needed for them to work.</p>
<pre><code>if (!isMicrophoneAvailable) {
  // Render some fallback content
}
</code></pre>
<p><a name="SpeechRecognition"></a></p>
<h2>SpeechRecognition</h2>
<p>Object providing functions to manage the global state of the microphone. Import with:</p>
<pre><code>import SpeechRecognition from 'react-speech-recognition'
</code></pre>
<h3>Functions</h3>
<h4>startListening (async)</h4>
<p>Start listening to speech.</p>
<pre><code>SpeechRecognition.startListening()
</code></pre>
<p>This is an asynchronous function, so it will need to be awaited if you want to do something after the microphone has been turned on.</p>
<p>It can be called with an options argument. For example:</p>
<pre><code>SpeechRecognition.startListening({
  continuous: true,
  language: 'zh-CN'
})
</code></pre>
<p>The following options are available:</p>
<h5>continuous [bool]</h5>
<p>By default, the microphone will stop listening when the user stops speaking (<code>continuous: false</code>). This reflects the approach taken by "press to talk" buttons on modern devices.</p>
<p>If you want to listen continuously, set the <code>continuous</code> property to <code>true</code> when calling <code>startListening</code>. The microphone will continue to listen, even after the user has stopped speaking.</p>
<pre><code>SpeechRecognition.startListening({ continuous: true })
</code></pre>
<p><a name="language"></a></p>
<h5>language [string]</h5>
<p>To listen for a specific language, you can pass a language tag (e.g. <code>'zh-CN'</code> for Chinese) when calling <code>startListening</code>.</p>
<pre><code>SpeechRecognition.startListening({ language: 'zh-CN' })
</code></pre>
<p>Some known supported languages (based on <a href="http://stackoverflow.com/a/14302134/338039">this Stack Overflow post</a>):</p>
<ul>
<li>Afrikaans <code>af</code></li>
<li>Basque <code>eu</code></li>
<li>Bulgarian <code>bg</code></li>
<li>Catalan <code>ca</code></li>
<li>Arabic (Egypt) <code>ar-EG</code></li>
<li>Arabic (Jordan) <code>ar-JO</code></li>
<li>Arabic (Kuwait) <code>ar-KW</code></li>
<li>Arabic (Lebanon) <code>ar-LB</code></li>
<li>Arabic (Qatar) <code>ar-QA</code></li>
<li>Arabic (UAE) <code>ar-AE</code></li>
<li>Arabic (Morocco) <code>ar-MA</code></li>
<li>Arabic (Iraq) <code>ar-IQ</code></li>
<li>Arabic (Algeria) <code>ar-DZ</code></li>
<li>Arabic (Bahrain) <code>ar-BH</code></li>
<li>Arabic (Lybia) <code>ar-LY</code></li>
<li>Arabic (Oman) <code>ar-OM</code></li>
<li>Arabic (Saudi Arabia) <code>ar-SA</code></li>
<li>Arabic (Tunisia) <code>ar-TN</code></li>
<li>Arabic (Yemen) <code>ar-YE</code></li>
<li>Czech <code>cs</code></li>
<li>Dutch <code>nl-NL</code></li>
<li>English (Australia) <code>en-AU</code></li>
<li>English (Canada) <code>en-CA</code></li>
<li>English (India) <code>en-IN</code></li>
<li>English (New Zealand) <code>en-NZ</code></li>
<li>English (South Africa) <code>en-ZA</code></li>
<li>English(UK) <code>en-GB</code></li>
<li>English(US) <code>en-US</code></li>
<li>Finnish <code>fi</code></li>
<li>French <code>fr-FR</code></li>
<li>Galician <code>gl</code></li>
<li>German <code>de-DE</code></li>
<li>Greek  <code>el-GR</code></li>
<li>Hebrew <code>he</code></li>
<li>Hungarian <code>hu</code></li>
<li>Icelandic <code>is</code></li>
<li>Italian <code>it-IT</code></li>
<li>Indonesian <code>id</code></li>
<li>Japanese <code>ja</code></li>
<li>Korean <code>ko</code></li>
<li>Latin <code>la</code></li>
<li>Mandarin Chinese <code>zh-CN</code></li>
<li>Taiwanese <code>zh-TW</code></li>
<li>Cantonese <code>zh-HK</code></li>
<li>Malaysian <code>ms-MY</code></li>
<li>Norwegian <code>no-NO</code></li>
<li>Polish <code>pl</code></li>
<li>Pig Latin <code>xx-piglatin</code></li>
<li>Portuguese <code>pt-PT</code></li>
<li>Portuguese (Brasil) <code>pt-br</code></li>
<li>Romanian <code>ro-RO</code></li>
<li>Russian <code>ru</code></li>
<li>Serbian <code>sr-SP</code></li>
<li>Slovak <code>sk</code></li>
<li>Spanish (Argentina) <code>es-AR</code></li>
<li>Spanish (Bolivia) <code>es-BO</code></li>
<li>Spanish (Chile) <code>es-CL</code></li>
<li>Spanish (Colombia) <code>es-CO</code></li>
<li>Spanish (Costa Rica) <code>es-CR</code></li>
<li>Spanish (Dominican Republic) <code>es-DO</code></li>
<li>Spanish (Ecuador) <code>es-EC</code></li>
<li>Spanish (El Salvador) <code>es-SV</code></li>
<li>Spanish (Guatemala) <code>es-GT</code></li>
<li>Spanish (Honduras) <code>es-HN</code></li>
<li>Spanish (Mexico) <code>es-MX</code></li>
<li>Spanish (Nicaragua) <code>es-NI</code></li>
<li>Spanish (Panama) <code>es-PA</code></li>
<li>Spanish (Paraguay) <code>es-PY</code></li>
<li>Spanish (Peru) <code>es-PE</code></li>
<li>Spanish (Puerto Rico) <code>es-PR</code></li>
<li>Spanish (Spain) <code>es-ES</code></li>
<li>Spanish (US) <code>es-US</code></li>
<li>Spanish (Uruguay) <code>es-UY</code></li>
<li>Spanish (Venezuela) <code>es-VE</code></li>
<li>Swedish <code>sv-SE</code></li>
<li>Turkish <code>tr</code></li>
<li>Zulu <code>zu</code></li>
</ul>
<h4>stopListening (async)</h4>
<p>Turn the microphone off, but still finish processing any speech in progress.</p>
<pre><code>SpeechRecognition.stopListening()
</code></pre>
<p>This is an asynchronous function, so it will need to be awaited if you want to do something after the microphone has been turned off.</p>
<h4>abortListening (async)</h4>
<p>Turn the microphone off, and cancel the processing of any speech in progress.</p>
<pre><code>SpeechRecognition.abortListening()
</code></pre>
<p>This is an asynchronous function, so it will need to be awaited if you want to do something after the microphone has been turned off.</p>
<h4>getRecognition</h4>
<p>This returns the underlying <a href="https://developer.mozilla.org/en-US/docs/Web/API/SpeechRecognition">object</a> used by Web Speech API.</p>
<h4>applyPolyfill</h4>
<p>Replace the native Speech Recognition engine (if there is one) with a custom implementation of the <a href="https://wicg.github.io/speech-api/#speechreco-section">W3C SpeechRecognition specification</a>. If there is a Speech Recognition implementation already listening to the microphone, this will be turned off. See <a href="/polyfills">Polyfills</a> for more information on how to use this.</p>
<pre><code>SpeechRecognition.applyPolyfill(SpeechRecognitionPolyfill)
</code></pre>
</div></div></main></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"title":"React Speech Recognition API docs","contentHtml":"\u003ch2\u003eInterface\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#useSpeechRecognition\"\u003euseSpeechRecognition\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#SpeechRecognition\"\u003eSpeechRecognition\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca name=\"useSpeechRecognition\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003euseSpeechRecognition\u003c/h2\u003e\n\u003cp\u003eReact hook for consuming speech recorded by the microphone. Import with:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport { useSpeechRecognition } from 'react-speech-recognition'\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eInput props\u003c/h3\u003e\n\u003cp\u003eThese are passed as an object argument to \u003ccode\u003euseSpeechRecognition\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003euseSpeechRecognition({ transcribing, clearTranscriptOnListen, commands })\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003etranscribing [bool]\u003c/h4\u003e\n\u003cp\u003eIs this component collecting a transcript or not? This is independent of the global \u003ccode\u003elistening\u003c/code\u003e state of the microphone. \u003ccode\u003etrue\u003c/code\u003e by default.\u003c/p\u003e\n\u003ch4\u003eclearTranscriptOnListen [bool]\u003c/h4\u003e\n\u003cp\u003eDoes this component clear its transcript when the microphone is turned on? Has no effect when continuous listening is enabled. \u003ccode\u003etrue\u003c/code\u003e by default.\u003c/p\u003e\n\u003ch4\u003ecommands [list]\u003c/h4\u003e\n\u003cp\u003eSee \u003ca href=\"/react-speech-recognition#commands\"\u003eCommands\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003eOutput state\u003c/h3\u003e\n\u003cp\u003eThese are returned from \u003ccode\u003euseSpeechRecognition\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  const {\n    transcript,\n    interimTranscript,\n    finalTranscript,\n    resetTranscript,\n    listening,\n    browserSupportsSpeechRecognition,\n    isMicrophoneAvailable,\n  } = useSpeechRecognition()\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003etranscript [string]\u003c/h4\u003e\n\u003cp\u003eTranscription of all speech that has been spoken into the microphone. Is equivalent to the final transcript followed by the interim transcript, separated by a space.\u003c/p\u003e\n\u003ch4\u003eresetTranscript [function]\u003c/h4\u003e\n\u003cp\u003eSets \u003ccode\u003etranscript\u003c/code\u003e to an empty string.\u003c/p\u003e\n\u003ch4\u003elistening [bool]\u003c/h4\u003e\n\u003cp\u003eIf true, the Web Speech API is listening to speech from the microphone.\u003c/p\u003e\n\u003ch4\u003einterimTranscript [string]\u003c/h4\u003e\n\u003cp\u003eTranscription of speech that the Web Speech API is still processing (i.e. it's still deciding what has just been spoken).\u003c/p\u003e\n\u003cp\u003eFor the current words being spoken, the interim transcript reflects each successive guess made by the transcription algorithm. When the browserâ€™s confidence in its guess is maximized, it is added to the final transcript.\u003c/p\u003e\n\u003cp\u003eThe difference between interim and final transcripts can be illustrated by an example over four iterations of the transcription algorithm:\u003c/p\u003e\n\u003cimg src=\"images/transcript-table.png\" width=\"300\" alt=\"A series of transcripts that grow as more speech is transcribed\"\u003e\n\u003ch4\u003efinalTranscript [string]\u003c/h4\u003e\n\u003cp\u003eTranscription of speech that the Web Speech API has finished processing.\u003c/p\u003e\n\u003ch4\u003ebrowserSupportsSpeechRecognition [bool]\u003c/h4\u003e\n\u003cp\u003eThe Web Speech API is not supported on all browsers, so it is recommended that you render some fallback content if it is not supported by the user's browser:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eif (!browserSupportsSpeechRecognition) {\n  // Render some fallback content\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003ebrowserSupportsContinuousListening [bool]\u003c/h4\u003e\n\u003cp\u003eContinuous listening is not supported on all browsers, so it is recommended that you apply some fallback behaviour if your web app uses continuous listening and is running on a browser that doesn't support it:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eif (browserSupportsContinuousListening) {\n  SpeechRecognition.startListening({ continuous: true })\n} else {\n  // Fallback behaviour\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003eisMicrophoneAvailable [bool]\u003c/h4\u003e\n\u003cp\u003eThe user has to give permission for their microphone to be used before transcription can begin. They are asked for permission when \u003ccode\u003ereact-speech-recognition\u003c/code\u003e first tries to start listening. This state will become \u003ccode\u003efalse\u003c/code\u003e if they deny access. In this case, it's advised that you disable voice-driven features and indicate that microphone access is needed for them to work.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eif (!isMicrophoneAvailable) {\n  // Render some fallback content\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ca name=\"SpeechRecognition\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003eSpeechRecognition\u003c/h2\u003e\n\u003cp\u003eObject providing functions to manage the global state of the microphone. Import with:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport SpeechRecognition from 'react-speech-recognition'\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eFunctions\u003c/h3\u003e\n\u003ch4\u003estartListening (async)\u003c/h4\u003e\n\u003cp\u003eStart listening to speech.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eSpeechRecognition.startListening()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis is an asynchronous function, so it will need to be awaited if you want to do something after the microphone has been turned on.\u003c/p\u003e\n\u003cp\u003eIt can be called with an options argument. For example:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eSpeechRecognition.startListening({\n  continuous: true,\n  language: 'zh-CN'\n})\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe following options are available:\u003c/p\u003e\n\u003ch5\u003econtinuous [bool]\u003c/h5\u003e\n\u003cp\u003eBy default, the microphone will stop listening when the user stops speaking (\u003ccode\u003econtinuous: false\u003c/code\u003e). This reflects the approach taken by \"press to talk\" buttons on modern devices.\u003c/p\u003e\n\u003cp\u003eIf you want to listen continuously, set the \u003ccode\u003econtinuous\u003c/code\u003e property to \u003ccode\u003etrue\u003c/code\u003e when calling \u003ccode\u003estartListening\u003c/code\u003e. The microphone will continue to listen, even after the user has stopped speaking.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eSpeechRecognition.startListening({ continuous: true })\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ca name=\"language\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch5\u003elanguage [string]\u003c/h5\u003e\n\u003cp\u003eTo listen for a specific language, you can pass a language tag (e.g. \u003ccode\u003e'zh-CN'\u003c/code\u003e for Chinese) when calling \u003ccode\u003estartListening\u003c/code\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eSpeechRecognition.startListening({ language: 'zh-CN' })\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSome known supported languages (based on \u003ca href=\"http://stackoverflow.com/a/14302134/338039\"\u003ethis Stack Overflow post\u003c/a\u003e):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAfrikaans \u003ccode\u003eaf\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eBasque \u003ccode\u003eeu\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eBulgarian \u003ccode\u003ebg\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eCatalan \u003ccode\u003eca\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eArabic (Egypt) \u003ccode\u003ear-EG\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eArabic (Jordan) \u003ccode\u003ear-JO\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eArabic (Kuwait) \u003ccode\u003ear-KW\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eArabic (Lebanon) \u003ccode\u003ear-LB\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eArabic (Qatar) \u003ccode\u003ear-QA\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eArabic (UAE) \u003ccode\u003ear-AE\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eArabic (Morocco) \u003ccode\u003ear-MA\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eArabic (Iraq) \u003ccode\u003ear-IQ\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eArabic (Algeria) \u003ccode\u003ear-DZ\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eArabic (Bahrain) \u003ccode\u003ear-BH\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eArabic (Lybia) \u003ccode\u003ear-LY\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eArabic (Oman) \u003ccode\u003ear-OM\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eArabic (Saudi Arabia) \u003ccode\u003ear-SA\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eArabic (Tunisia) \u003ccode\u003ear-TN\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eArabic (Yemen) \u003ccode\u003ear-YE\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eCzech \u003ccode\u003ecs\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eDutch \u003ccode\u003enl-NL\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eEnglish (Australia) \u003ccode\u003een-AU\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eEnglish (Canada) \u003ccode\u003een-CA\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eEnglish (India) \u003ccode\u003een-IN\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eEnglish (New Zealand) \u003ccode\u003een-NZ\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eEnglish (South Africa) \u003ccode\u003een-ZA\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eEnglish(UK) \u003ccode\u003een-GB\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eEnglish(US) \u003ccode\u003een-US\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eFinnish \u003ccode\u003efi\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eFrench \u003ccode\u003efr-FR\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eGalician \u003ccode\u003egl\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eGerman \u003ccode\u003ede-DE\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eGreek  \u003ccode\u003eel-GR\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eHebrew \u003ccode\u003ehe\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eHungarian \u003ccode\u003ehu\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eIcelandic \u003ccode\u003eis\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eItalian \u003ccode\u003eit-IT\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eIndonesian \u003ccode\u003eid\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eJapanese \u003ccode\u003eja\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eKorean \u003ccode\u003eko\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eLatin \u003ccode\u003ela\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eMandarin Chinese \u003ccode\u003ezh-CN\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eTaiwanese \u003ccode\u003ezh-TW\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eCantonese \u003ccode\u003ezh-HK\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eMalaysian \u003ccode\u003ems-MY\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eNorwegian \u003ccode\u003eno-NO\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003ePolish \u003ccode\u003epl\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003ePig Latin \u003ccode\u003exx-piglatin\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003ePortuguese \u003ccode\u003ept-PT\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003ePortuguese (Brasil) \u003ccode\u003ept-br\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eRomanian \u003ccode\u003ero-RO\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eRussian \u003ccode\u003eru\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSerbian \u003ccode\u003esr-SP\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSlovak \u003ccode\u003esk\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Argentina) \u003ccode\u003ees-AR\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Bolivia) \u003ccode\u003ees-BO\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Chile) \u003ccode\u003ees-CL\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Colombia) \u003ccode\u003ees-CO\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Costa Rica) \u003ccode\u003ees-CR\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Dominican Republic) \u003ccode\u003ees-DO\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Ecuador) \u003ccode\u003ees-EC\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (El Salvador) \u003ccode\u003ees-SV\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Guatemala) \u003ccode\u003ees-GT\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Honduras) \u003ccode\u003ees-HN\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Mexico) \u003ccode\u003ees-MX\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Nicaragua) \u003ccode\u003ees-NI\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Panama) \u003ccode\u003ees-PA\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Paraguay) \u003ccode\u003ees-PY\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Peru) \u003ccode\u003ees-PE\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Puerto Rico) \u003ccode\u003ees-PR\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Spain) \u003ccode\u003ees-ES\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (US) \u003ccode\u003ees-US\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Uruguay) \u003ccode\u003ees-UY\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Venezuela) \u003ccode\u003ees-VE\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSwedish \u003ccode\u003esv-SE\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eTurkish \u003ccode\u003etr\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eZulu \u003ccode\u003ezu\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003estopListening (async)\u003c/h4\u003e\n\u003cp\u003eTurn the microphone off, but still finish processing any speech in progress.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eSpeechRecognition.stopListening()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis is an asynchronous function, so it will need to be awaited if you want to do something after the microphone has been turned off.\u003c/p\u003e\n\u003ch4\u003eabortListening (async)\u003c/h4\u003e\n\u003cp\u003eTurn the microphone off, and cancel the processing of any speech in progress.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eSpeechRecognition.abortListening()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis is an asynchronous function, so it will need to be awaited if you want to do something after the microphone has been turned off.\u003c/p\u003e\n\u003ch4\u003egetRecognition\u003c/h4\u003e\n\u003cp\u003eThis returns the underlying \u003ca href=\"https://developer.mozilla.org/en-US/docs/Web/API/SpeechRecognition\"\u003eobject\u003c/a\u003e used by Web Speech API.\u003c/p\u003e\n\u003ch4\u003eapplyPolyfill\u003c/h4\u003e\n\u003cp\u003eReplace the native Speech Recognition engine (if there is one) with a custom implementation of the \u003ca href=\"https://wicg.github.io/speech-api/#speechreco-section\"\u003eW3C SpeechRecognition specification\u003c/a\u003e. If there is a Speech Recognition implementation already listening to the microphone, this will be turned off. See \u003ca href=\"/polyfills\"\u003ePolyfills\u003c/a\u003e for more information on how to use this.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eSpeechRecognition.applyPolyfill(SpeechRecognitionPolyfill)\n\u003c/code\u003e\u003c/pre\u003e\n"},"__N_SSG":true},"page":"/react-speech-recognition-api-docs","query":{},"buildId":"vDOyqE9dTMogwuLDZy0OQ","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>