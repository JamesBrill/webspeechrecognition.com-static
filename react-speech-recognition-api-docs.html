<!DOCTYPE html><html><head><meta charSet="utf-8"/><title>React Speech Recognition API docs</title><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="icon" href="/favicon.ico"/><meta name="next-head-count" content="4"/><link rel="preload" href="/_next/static/css/fcd984c04d57235acdaa.css" as="style"/><link rel="stylesheet" href="/_next/static/css/fcd984c04d57235acdaa.css" data-n-g=""/><link rel="preload" href="/_next/static/css/804ad107cac14cc49c60.css" as="style"/><link rel="stylesheet" href="/_next/static/css/804ad107cac14cc49c60.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-a40ef1678bae11e696dba45124eadd70.js"></script><script src="/_next/static/chunks/webpack-f47d69457824065d04c3.js" defer=""></script><script src="/_next/static/chunks/framework-1a85486469afb3278dba.js" defer=""></script><script src="/_next/static/chunks/main-75772fcf59f94cd717f4.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6f1364eda477534ba135.js" defer=""></script><script src="/_next/static/chunks/pages/react-speech-recognition-api-docs-5c79b6ac170552f948be.js" defer=""></script><script src="/_next/static/vDOyqE9dTMogwuLDZy0OQ/_buildManifest.js" defer=""></script><script src="/_next/static/vDOyqE9dTMogwuLDZy0OQ/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="Layout_Layout__2MSiz"><main><h1>React Speech Recognition API docs</h1><div class="Article_Article__2h4f-"><a href="/">Back to home</a><div class="Article_Article__content__Vy2r-"><h2>Interface</h2>
<ul>
<li><a href="#useSpeechRecognition">useSpeechRecognition</a></li>
<li><a href="#SpeechRecognition">SpeechRecognition</a></li>
</ul>
<p><a name="useSpeechRecognition"></a></p>
<h2>useSpeechRecognition</h2>
<p>React hook for consuming speech recorded by the microphone. Import with:</p>
<pre><code>import { useSpeechRecognition } from 'react-speech-recognition'
</code></pre>
<h3>Input props</h3>
<p>These are passed as an object argument to <code>useSpeechRecognition</code>:</p>
<pre><code>useSpeechRecognition({ transcribing, clearTranscriptOnListen, commands })
</code></pre>
<h4>transcribing [bool]</h4>
<p>Is this component collecting a transcript or not? This is independent of the global <code>listening</code> state of the microphone. <code>true</code> by default.</p>
<h4>clearTranscriptOnListen [bool]</h4>
<p>Does this component clear its transcript when the microphone is turned on? Has no effect when continuous listening is enabled. <code>true</code> by default.</p>
<h4>commands [list]</h4>
<p>See <a href="/react-speech-recognition#commands">Commands</a>.</p>
<h3>Output state</h3>
<p>These are returned from <code>useSpeechRecognition</code>:</p>
<pre><code>  const {
    transcript,
    interimTranscript,
    finalTranscript,
    resetTranscript,
    listening,
    browserSupportsSpeechRecognition,
    isMicrophoneAvailable,
  } = useSpeechRecognition()
</code></pre>
<h4>transcript [string]</h4>
<p>Transcription of all speech that has been spoken into the microphone. Is equivalent to the final transcript followed by the interim transcript, separated by a space.</p>
<h4>resetTranscript [function]</h4>
<p>Sets <code>transcript</code> to an empty string.</p>
<h4>listening [bool]</h4>
<p>If true, the Web Speech API is listening to speech from the microphone.</p>
<h4>interimTranscript [string]</h4>
<p>Transcription of speech that the Web Speech API is still processing (i.e. it's still deciding what has just been spoken).</p>
<p>For the current words being spoken, the interim transcript reflects each successive guess made by the transcription algorithm. When the browser’s confidence in its guess is maximized, it is added to the final transcript.</p>
<p>The difference between interim and final transcripts can be illustrated by an example over four iterations of the transcription algorithm:</p>
<img src="images/transcript-table.png" width="300" alt="A series of transcripts that grow as more speech is transcribed">
<h4>finalTranscript [string]</h4>
<p>Transcription of speech that the Web Speech API has finished processing.</p>
<h4>browserSupportsSpeechRecognition [bool]</h4>
<p>The Web Speech API is not supported on all browsers, so it is recommended that you render some fallback content if it is not supported by the user's browser:</p>
<pre><code>if (!browserSupportsSpeechRecognition) {
  // Render some fallback content
}
</code></pre>
<h4>browserSupportsContinuousListening [bool]</h4>
<p>Continuous listening is not supported on all browsers, so it is recommended that you apply some fallback behaviour if your web app uses continuous listening and is running on a browser that doesn't support it:</p>
<pre><code>if (browserSupportsContinuousListening) {
  SpeechRecognition.startListening({ continuous: true })
} else {
  // Fallback behaviour
}
</code></pre>
<h4>isMicrophoneAvailable [bool]</h4>
<p>The user has to give permission for their microphone to be used before transcription can begin. They are asked for permission when <code>react-speech-recognition</code> first tries to start listening. This state will become <code>false</code> if they deny access. In this case, it's advised that you disable voice-driven features and indicate that microphone access is needed for them to work.</p>
<pre><code>if (!isMicrophoneAvailable) {
  // Render some fallback content
}
</code></pre>
<p><a name="SpeechRecognition"></a></p>
<h2>SpeechRecognition</h2>
<p>Object providing functions to manage the global state of the microphone. Import with:</p>
<pre><code>import SpeechRecognition from 'react-speech-recognition'
</code></pre>
<h3>Functions</h3>
<h4>startListening (async)</h4>
<p>Start listening to speech.</p>
<pre><code>SpeechRecognition.startListening()
</code></pre>
<p>This is an asynchronous function, so it will need to be awaited if you want to do something after the microphone has been turned on.</p>
<p>It can be called with an options argument. For example:</p>
<pre><code>SpeechRecognition.startListening({
  continuous: true,
  language: 'zh-CN'
})
</code></pre>
<p>The following options are available:</p>
<h5>continuous [bool]</h5>
<p>By default, the microphone will stop listening when the user stops speaking (<code>continuous: false</code>). This reflects the approach taken by "press to talk" buttons on modern devices.</p>
<p>If you want to listen continuously, set the <code>continuous</code> property to <code>true</code> when calling <code>startListening</code>. The microphone will continue to listen, even after the user has stopped speaking.</p>
<pre><code>SpeechRecognition.startListening({ continuous: true })
</code></pre>
<p><a name="language"></a></p>
<h5>language [string]</h5>
<p>To listen for a specific language, you can pass a language tag (e.g. <code>'zh-CN'</code> for Chinese) when calling <code>startListening</code>.</p>
<pre><code>SpeechRecognition.startListening({ language: 'zh-CN' })
</code></pre>
<p>Some known supported languages (based on <a href="http://stackoverflow.com/a/14302134/338039">this Stack Overflow post</a>):</p>
<ul>
<li>Afrikaans <code>af</code></li>
<li>Basque <code>eu</code></li>
<li>Bulgarian <code>bg</code></li>
<li>Catalan <code>ca</code></li>
<li>Arabic (Egypt) <code>ar-EG</code></li>
<li>Arabic (Jordan) <code>ar-JO</code></li>
<li>Arabic (Kuwait) <code>ar-KW</code></li>
<li>Arabic (Lebanon) <code>ar-LB</code></li>
<li>Arabic (Qatar) <code>ar-QA</code></li>
<li>Arabic (UAE) <code>ar-AE</code></li>
<li>Arabic (Morocco) <code>ar-MA</code></li>
<li>Arabic (Iraq) <code>ar-IQ</code></li>
<li>Arabic (Algeria) <code>ar-DZ</code></li>
<li>Arabic (Bahrain) <code>ar-BH</code></li>
<li>Arabic (Lybia) <code>ar-LY</code></li>
<li>Arabic (Oman) <code>ar-OM</code></li>
<li>Arabic (Saudi Arabia) <code>ar-SA</code></li>
<li>Arabic (Tunisia) <code>ar-TN</code></li>
<li>Arabic (Yemen) <code>ar-YE</code></li>
<li>Czech <code>cs</code></li>
<li>Dutch <code>nl-NL</code></li>
<li>English (Australia) <code>en-AU</code></li>
<li>English (Canada) <code>en-CA</code></li>
<li>English (India) <code>en-IN</code></li>
<li>English (New Zealand) <code>en-NZ</code></li>
<li>English (South Africa) <code>en-ZA</code></li>
<li>English(UK) <code>en-GB</code></li>
<li>English(US) <code>en-US</code></li>
<li>Finnish <code>fi</code></li>
<li>French <code>fr-FR</code></li>
<li>Galician <code>gl</code></li>
<li>German <code>de-DE</code></li>
<li>Greek  <code>el-GR</code></li>
<li>Hebrew <code>he</code></li>
<li>Hungarian <code>hu</code></li>
<li>Icelandic <code>is</code></li>
<li>Italian <code>it-IT</code></li>
<li>Indonesian <code>id</code></li>
<li>Japanese <code>ja</code></li>
<li>Korean <code>ko</code></li>
<li>Latin <code>la</code></li>
<li>Mandarin Chinese <code>zh-CN</code></li>
<li>Taiwanese <code>zh-TW</code></li>
<li>Cantonese <code>zh-HK</code></li>
<li>Malaysian <code>ms-MY</code></li>
<li>Norwegian <code>no-NO</code></li>
<li>Polish <code>pl</code></li>
<li>Pig Latin <code>xx-piglatin</code></li>
<li>Portuguese <code>pt-PT</code></li>
<li>Portuguese (Brasil) <code>pt-br</code></li>
<li>Romanian <code>ro-RO</code></li>
<li>Russian <code>ru</code></li>
<li>Serbian <code>sr-SP</code></li>
<li>Slovak <code>sk</code></li>
<li>Spanish (Argentina) <code>es-AR</code></li>
<li>Spanish (Bolivia) <code>es-BO</code></li>
<li>Spanish (Chile) <code>es-CL</code></li>
<li>Spanish (Colombia) <code>es-CO</code></li>
<li>Spanish (Costa Rica) <code>es-CR</code></li>
<li>Spanish (Dominican Republic) <code>es-DO</code></li>
<li>Spanish (Ecuador) <code>es-EC</code></li>
<li>Spanish (El Salvador) <code>es-SV</code></li>
<li>Spanish (Guatemala) <code>es-GT</code></li>
<li>Spanish (Honduras) <code>es-HN</code></li>
<li>Spanish (Mexico) <code>es-MX</code></li>
<li>Spanish (Nicaragua) <code>es-NI</code></li>
<li>Spanish (Panama) <code>es-PA</code></li>
<li>Spanish (Paraguay) <code>es-PY</code></li>
<li>Spanish (Peru) <code>es-PE</code></li>
<li>Spanish (Puerto Rico) <code>es-PR</code></li>
<li>Spanish (Spain) <code>es-ES</code></li>
<li>Spanish (US) <code>es-US</code></li>
<li>Spanish (Uruguay) <code>es-UY</code></li>
<li>Spanish (Venezuela) <code>es-VE</code></li>
<li>Swedish <code>sv-SE</code></li>
<li>Turkish <code>tr</code></li>
<li>Zulu <code>zu</code></li>
</ul>
<h4>stopListening (async)</h4>
<p>Turn the microphone off, but still finish processing any speech in progress.</p>
<pre><code>SpeechRecognition.stopListening()
</code></pre>
<p>This is an asynchronous function, so it will need to be awaited if you want to do something after the microphone has been turned off.</p>
<h4>abortListening (async)</h4>
<p>Turn the microphone off, and cancel the processing of any speech in progress.</p>
<pre><code>SpeechRecognition.abortListening()
</code></pre>
<p>This is an asynchronous function, so it will need to be awaited if you want to do something after the microphone has been turned off.</p>
<h4>getRecognition</h4>
<p>This returns the underlying <a href="https://developer.mozilla.org/en-US/docs/Web/API/SpeechRecognition">object</a> used by Web Speech API.</p>
<h4>applyPolyfill</h4>
<p>Replace the native Speech Recognition engine (if there is one) with a custom implementation of the <a href="https://wicg.github.io/speech-api/#speechreco-section">W3C SpeechRecognition specification</a>. If there is a Speech Recognition implementation already listening to the microphone, this will be turned off. See <a href="/polyfills">Polyfills</a> for more information on how to use this.</p>
<pre><code>SpeechRecognition.applyPolyfill(SpeechRecognitionPolyfill)
</code></pre>
</div></div></main></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"title":"React Speech Recognition API docs","contentHtml":"\u003ch2\u003eInterface\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#useSpeechRecognition\"\u003euseSpeechRecognition\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#SpeechRecognition\"\u003eSpeechRecognition\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca name=\"useSpeechRecognition\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003euseSpeechRecognition\u003c/h2\u003e\n\u003cp\u003eReact hook for consuming speech recorded by the microphone. Import with:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport { useSpeechRecognition } from 'react-speech-recognition'\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eInput props\u003c/h3\u003e\n\u003cp\u003eThese are passed as an object argument to \u003ccode\u003euseSpeechRecognition\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003euseSpeechRecognition({ transcribing, clearTranscriptOnListen, commands })\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003etranscribing [bool]\u003c/h4\u003e\n\u003cp\u003eIs this component collecting a transcript or not? This is independent of the global \u003ccode\u003elistening\u003c/code\u003e state of the microphone. \u003ccode\u003etrue\u003c/code\u003e by default.\u003c/p\u003e\n\u003ch4\u003eclearTranscriptOnListen [bool]\u003c/h4\u003e\n\u003cp\u003eDoes this component clear its transcript when the microphone is turned on? Has no effect when continuous listening is enabled. \u003ccode\u003etrue\u003c/code\u003e by default.\u003c/p\u003e\n\u003ch4\u003ecommands [list]\u003c/h4\u003e\n\u003cp\u003eSee \u003ca href=\"/react-speech-recognition#commands\"\u003eCommands\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003eOutput state\u003c/h3\u003e\n\u003cp\u003eThese are returned from \u003ccode\u003euseSpeechRecognition\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  const {\n    transcript,\n    interimTranscript,\n    finalTranscript,\n    resetTranscript,\n    listening,\n    browserSupportsSpeechRecognition,\n    isMicrophoneAvailable,\n  } = useSpeechRecognition()\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003etranscript [string]\u003c/h4\u003e\n\u003cp\u003eTranscription of all speech that has been spoken into the microphone. Is equivalent to the final transcript followed by the interim transcript, separated by a space.\u003c/p\u003e\n\u003ch4\u003eresetTranscript [function]\u003c/h4\u003e\n\u003cp\u003eSets \u003ccode\u003etranscript\u003c/code\u003e to an empty string.\u003c/p\u003e\n\u003ch4\u003elistening [bool]\u003c/h4\u003e\n\u003cp\u003eIf true, the Web Speech API is listening to speech from the microphone.\u003c/p\u003e\n\u003ch4\u003einterimTranscript [string]\u003c/h4\u003e\n\u003cp\u003eTranscription of speech that the Web Speech API is still processing (i.e. it's still deciding what has just been spoken).\u003c/p\u003e\n\u003cp\u003eFor the current words being spoken, the interim transcript reflects each successive guess made by the transcription algorithm. When the browser’s confidence in its guess is maximized, it is added to the final transcript.\u003c/p\u003e\n\u003cp\u003eThe difference between interim and final transcripts can be illustrated by an example over four iterations of the transcription algorithm:\u003c/p\u003e\n\u003cimg src=\"images/transcript-table.png\" width=\"300\" alt=\"A series of transcripts that grow as more speech is transcribed\"\u003e\n\u003ch4\u003efinalTranscript [string]\u003c/h4\u003e\n\u003cp\u003eTranscription of speech that the Web Speech API has finished processing.\u003c/p\u003e\n\u003ch4\u003ebrowserSupportsSpeechRecognition [bool]\u003c/h4\u003e\n\u003cp\u003eThe Web Speech API is not supported on all browsers, so it is recommended that you render some fallback content if it is not supported by the user's browser:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eif (!browserSupportsSpeechRecognition) {\n  // Render some fallback content\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003ebrowserSupportsContinuousListening [bool]\u003c/h4\u003e\n\u003cp\u003eContinuous listening is not supported on all browsers, so it is recommended that you apply some fallback behaviour if your web app uses continuous listening and is running on a browser that doesn't support it:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eif (browserSupportsContinuousListening) {\n  SpeechRecognition.startListening({ continuous: true })\n} else {\n  // Fallback behaviour\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003eisMicrophoneAvailable [bool]\u003c/h4\u003e\n\u003cp\u003eThe user has to give permission for their microphone to be used before transcription can begin. They are asked for permission when \u003ccode\u003ereact-speech-recognition\u003c/code\u003e first tries to start listening. This state will become \u003ccode\u003efalse\u003c/code\u003e if they deny access. In this case, it's advised that you disable voice-driven features and indicate that microphone access is needed for them to work.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eif (!isMicrophoneAvailable) {\n  // Render some fallback content\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ca name=\"SpeechRecognition\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003eSpeechRecognition\u003c/h2\u003e\n\u003cp\u003eObject providing functions to manage the global state of the microphone. Import with:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport SpeechRecognition from 'react-speech-recognition'\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eFunctions\u003c/h3\u003e\n\u003ch4\u003estartListening (async)\u003c/h4\u003e\n\u003cp\u003eStart listening to speech.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eSpeechRecognition.startListening()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis is an asynchronous function, so it will need to be awaited if you want to do something after the microphone has been turned on.\u003c/p\u003e\n\u003cp\u003eIt can be called with an options argument. For example:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eSpeechRecognition.startListening({\n  continuous: true,\n  language: 'zh-CN'\n})\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe following options are available:\u003c/p\u003e\n\u003ch5\u003econtinuous [bool]\u003c/h5\u003e\n\u003cp\u003eBy default, the microphone will stop listening when the user stops speaking (\u003ccode\u003econtinuous: false\u003c/code\u003e). This reflects the approach taken by \"press to talk\" buttons on modern devices.\u003c/p\u003e\n\u003cp\u003eIf you want to listen continuously, set the \u003ccode\u003econtinuous\u003c/code\u003e property to \u003ccode\u003etrue\u003c/code\u003e when calling \u003ccode\u003estartListening\u003c/code\u003e. The microphone will continue to listen, even after the user has stopped speaking.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eSpeechRecognition.startListening({ continuous: true })\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ca name=\"language\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch5\u003elanguage [string]\u003c/h5\u003e\n\u003cp\u003eTo listen for a specific language, you can pass a language tag (e.g. \u003ccode\u003e'zh-CN'\u003c/code\u003e for Chinese) when calling \u003ccode\u003estartListening\u003c/code\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eSpeechRecognition.startListening({ language: 'zh-CN' })\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSome known supported languages (based on \u003ca href=\"http://stackoverflow.com/a/14302134/338039\"\u003ethis Stack Overflow post\u003c/a\u003e):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAfrikaans \u003ccode\u003eaf\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eBasque \u003ccode\u003eeu\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eBulgarian \u003ccode\u003ebg\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eCatalan \u003ccode\u003eca\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eArabic (Egypt) \u003ccode\u003ear-EG\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eArabic (Jordan) \u003ccode\u003ear-JO\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eArabic (Kuwait) \u003ccode\u003ear-KW\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eArabic (Lebanon) \u003ccode\u003ear-LB\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eArabic (Qatar) \u003ccode\u003ear-QA\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eArabic (UAE) \u003ccode\u003ear-AE\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eArabic (Morocco) \u003ccode\u003ear-MA\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eArabic (Iraq) \u003ccode\u003ear-IQ\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eArabic (Algeria) \u003ccode\u003ear-DZ\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eArabic (Bahrain) \u003ccode\u003ear-BH\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eArabic (Lybia) \u003ccode\u003ear-LY\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eArabic (Oman) \u003ccode\u003ear-OM\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eArabic (Saudi Arabia) \u003ccode\u003ear-SA\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eArabic (Tunisia) \u003ccode\u003ear-TN\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eArabic (Yemen) \u003ccode\u003ear-YE\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eCzech \u003ccode\u003ecs\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eDutch \u003ccode\u003enl-NL\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eEnglish (Australia) \u003ccode\u003een-AU\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eEnglish (Canada) \u003ccode\u003een-CA\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eEnglish (India) \u003ccode\u003een-IN\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eEnglish (New Zealand) \u003ccode\u003een-NZ\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eEnglish (South Africa) \u003ccode\u003een-ZA\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eEnglish(UK) \u003ccode\u003een-GB\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eEnglish(US) \u003ccode\u003een-US\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eFinnish \u003ccode\u003efi\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eFrench \u003ccode\u003efr-FR\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eGalician \u003ccode\u003egl\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eGerman \u003ccode\u003ede-DE\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eGreek  \u003ccode\u003eel-GR\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eHebrew \u003ccode\u003ehe\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eHungarian \u003ccode\u003ehu\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eIcelandic \u003ccode\u003eis\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eItalian \u003ccode\u003eit-IT\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eIndonesian \u003ccode\u003eid\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eJapanese \u003ccode\u003eja\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eKorean \u003ccode\u003eko\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eLatin \u003ccode\u003ela\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eMandarin Chinese \u003ccode\u003ezh-CN\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eTaiwanese \u003ccode\u003ezh-TW\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eCantonese \u003ccode\u003ezh-HK\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eMalaysian \u003ccode\u003ems-MY\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eNorwegian \u003ccode\u003eno-NO\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003ePolish \u003ccode\u003epl\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003ePig Latin \u003ccode\u003exx-piglatin\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003ePortuguese \u003ccode\u003ept-PT\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003ePortuguese (Brasil) \u003ccode\u003ept-br\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eRomanian \u003ccode\u003ero-RO\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eRussian \u003ccode\u003eru\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSerbian \u003ccode\u003esr-SP\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSlovak \u003ccode\u003esk\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Argentina) \u003ccode\u003ees-AR\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Bolivia) \u003ccode\u003ees-BO\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Chile) \u003ccode\u003ees-CL\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Colombia) \u003ccode\u003ees-CO\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Costa Rica) \u003ccode\u003ees-CR\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Dominican Republic) \u003ccode\u003ees-DO\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Ecuador) \u003ccode\u003ees-EC\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (El Salvador) \u003ccode\u003ees-SV\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Guatemala) \u003ccode\u003ees-GT\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Honduras) \u003ccode\u003ees-HN\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Mexico) \u003ccode\u003ees-MX\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Nicaragua) \u003ccode\u003ees-NI\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Panama) \u003ccode\u003ees-PA\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Paraguay) \u003ccode\u003ees-PY\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Peru) \u003ccode\u003ees-PE\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Puerto Rico) \u003ccode\u003ees-PR\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Spain) \u003ccode\u003ees-ES\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (US) \u003ccode\u003ees-US\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Uruguay) \u003ccode\u003ees-UY\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSpanish (Venezuela) \u003ccode\u003ees-VE\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSwedish \u003ccode\u003esv-SE\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eTurkish \u003ccode\u003etr\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eZulu \u003ccode\u003ezu\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003estopListening (async)\u003c/h4\u003e\n\u003cp\u003eTurn the microphone off, but still finish processing any speech in progress.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eSpeechRecognition.stopListening()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis is an asynchronous function, so it will need to be awaited if you want to do something after the microphone has been turned off.\u003c/p\u003e\n\u003ch4\u003eabortListening (async)\u003c/h4\u003e\n\u003cp\u003eTurn the microphone off, and cancel the processing of any speech in progress.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eSpeechRecognition.abortListening()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis is an asynchronous function, so it will need to be awaited if you want to do something after the microphone has been turned off.\u003c/p\u003e\n\u003ch4\u003egetRecognition\u003c/h4\u003e\n\u003cp\u003eThis returns the underlying \u003ca href=\"https://developer.mozilla.org/en-US/docs/Web/API/SpeechRecognition\"\u003eobject\u003c/a\u003e used by Web Speech API.\u003c/p\u003e\n\u003ch4\u003eapplyPolyfill\u003c/h4\u003e\n\u003cp\u003eReplace the native Speech Recognition engine (if there is one) with a custom implementation of the \u003ca href=\"https://wicg.github.io/speech-api/#speechreco-section\"\u003eW3C SpeechRecognition specification\u003c/a\u003e. If there is a Speech Recognition implementation already listening to the microphone, this will be turned off. See \u003ca href=\"/polyfills\"\u003ePolyfills\u003c/a\u003e for more information on how to use this.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eSpeechRecognition.applyPolyfill(SpeechRecognitionPolyfill)\n\u003c/code\u003e\u003c/pre\u003e\n"},"__N_SSG":true},"page":"/react-speech-recognition-api-docs","query":{},"buildId":"vDOyqE9dTMogwuLDZy0OQ","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>